{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129d5ede",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_main_arcada.png\" style=\"width:1400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd649db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Foundational Models 1/3  \n",
    "## Introduction\n",
    "\n",
    "#### Presenter:\n",
    "> Anton Akusok (PhD), **Senior Lecturer in IT / Senior Data engineer at Zalando**<br/> \n",
    "(*email*: anton.akusok@arcada.fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895bc5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "* What are Fondational models\n",
    "* Why they exist\n",
    "* How to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cfc24b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2aff2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We wrote a paper on fake signature recognition 6 years ago\n",
    "\n",
    "<img src=\"./images/f1-signatures.png\" alt=\"Signatures\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356448a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Original authors of the dataset created and trained a complex deep learning network for signatures verification\n",
    "\n",
    "<img src=\"./images/f1-net-original.png\" alt=\"Original net\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96286e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We got marginally better results without training any deep learning models.\n",
    "\n",
    "### How?\n",
    "\n",
    "<img src=\"./images/f1-inception.png\" alt=\"Original net\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1537af3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead of creating and training a DL model specifically for our task, we used an existing DL model trained for image classification: a **foundational model**.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "* signatures data has 30 signatures for 10,000 \"people\" for 300,000 samples and 2 classes\n",
    "* \"Inception21k: is trained on 14 mln. images with 21,000 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6947657",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are Foundational Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703d6a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *Foundation models are AI neural networks trained on massive unlabeled datasets to handle a wide variety of jobs from translating text to analyzing medical images.*  \n",
    "(NVidia, https://blogs.nvidia.com/blog/what-are-foundation-models/)\n",
    "\n",
    "They learn the data itself, and make handling this or similar data much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac85792",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./images/f1-models.jpg\" alt=\"Models\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba603329",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What makes Foundational models different from DL or ML models?\n",
    "\n",
    "* trained on mountains of raw data\n",
    "* foundational model is per \"type of data\"\n",
    "    * regular DL or ML model is per \"task\"\n",
    "* mostly with unsupervised learning (no labels)\n",
    "* but data enrichments can apply: \n",
    "    * human validated answers for LLM\n",
    "    * image + caption for image models\n",
    "    * task caption + video recording for robot training\n",
    "* can be adapted to a broad range of tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9053dccb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Foundational models exist?\n",
    "\n",
    "Because data is hard, and foundational models make data easy.\n",
    "\n",
    "Mathematically, they learn the data manifold.  \n",
    "(https://prateekvjoshi.com/2014/06/21/what-is-manifold-learning/)\n",
    "\n",
    "<img src=\"./images/f1-swissroll.png\" alt=\"Models\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43623d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Foundational models make for very cheap solutions\n",
    "\n",
    "Models themselves are ridiculously expensive to train, but they are trained only once.\n",
    "<img src=\"./images/f1-training.jpg\" alt=\"Training\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e02c82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The expensive training is necessary to get good results on complex data like text of images.  \n",
    "\n",
    "This prevented creating universal text/image models until a few years ago.\n",
    "\n",
    "Human equivalent:\n",
    "\n",
    "* foundational human vision model: evolving human being from amoeba\n",
    "* task adaptation: learning how to read\n",
    "\n",
    "Evolving a new human being for every class in school would be too expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585cd25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Foundational models can learn from two types of data\n",
    "\n",
    "Two data types in one model makes some very useful models!\n",
    "\n",
    "* text + image: image generation\n",
    "* text + audio: text-to-speech, song and soundtrack synthesis\n",
    "* text + shape: 3D object recognition and generation\n",
    "* text + video: robot task planning, robot actions generation from request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed5c54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What types of Foundational models exist?\n",
    "\n",
    "Foundational models can be grouped by distinctive \"features\", but there is no hierarchy and one model can have several \"features\"  \n",
    "https://github.com/awaisrauf/Awesome-CV-Foundational-Models\n",
    "\n",
    "<img src=\"./images/f1-taxonomy-1.png\" alt=\"Models\" width=\"90%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f7fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some architecture styles of vision models\n",
    "\n",
    "Models re-use standard blocks in different combinations; researchers write NN layers, backpropagation rules, etc. to implement these blocks as one model trainable on GPUs.\n",
    "\n",
    "<img src=\"./images/f1-taxonomy-2.png\" alt=\"Models\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8e17",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature: Autoregressive models\n",
    "\n",
    "\"Autoregressive\" model predicts one next step from previous steps.\n",
    "\n",
    "Canonical example: LLMs that generate text 1 word at a time\n",
    "\n",
    "This is why LLMs are \"slow\" - they cannot generate a batch of 1000 words at once, the whole model repeats for each word independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5dc2ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The term **autoregressive** is very old, refers to an old linear regression model where inputs are `X[:-1]` and targets are `X[1:]` so the model is doing regression upon itself - since the \"auto\"regressive.\n",
    "\n",
    "LLMs and modern models have **state** that is carried from one sample to the next, slowly changing over time.\n",
    "\n",
    "**Attention** are weights that connect one sample to select previous samples instead of all previous samples.\n",
    "\n",
    "<img src=\"./images/f1-autoregressive.png\" alt=\"Models\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa908e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature: Diffusion models\n",
    "https://stable-diffusion-art.com/how-stable-diffusion-work/\n",
    "\n",
    "\n",
    "Widely used in image generation, and can generate new image from existing sample image:\n",
    "\n",
    "* super-resolution (literally drawing new image from low-res sample image)\n",
    "* inpainting (drawing masked parts of an image; can extend image outwards making up its surroundings)\n",
    "* specific image improvements guided by text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a250d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is the craziest model idea because it starts with noise, **predicts noise**, and adds noise to noise until an image is developed like a photo...\n",
    "\n",
    "<img src=\"./images/f1-diffusion.png\" alt=\"Models\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b8497",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature: Variational Autoencoder\n",
    "\n",
    "A model that *compresses* data to a lower-dimensional vector, and *decompresses* back.\n",
    "\n",
    "Does not have enough information to store raw data so it learns to improvise and keep the important features only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e768f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Works together with diffusion model to enable denoising in small *latent* space.\n",
    "\n",
    "Enables very high-resolution images because work happens in resolution-agnostic latent space\n",
    "\n",
    "<img src=\"./images/f1-vae.png\" alt=\"Models\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dca330",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature: Generative Adversarial Network (GAN)\n",
    "\n",
    "A training approach rather than a network itself. Originally for image generation but useful in many other areas like anomaly detection.\n",
    "\n",
    "Can build your own simple GAN:  \n",
    "https://www.geeksforgeeks.org/generative-adversarial-network-gan/\n",
    "\n",
    "<img src=\"./images/f1-gan.png\" alt=\"Models\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbc74e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature: Contrastive Learning (multimodal models)\n",
    "\n",
    "Training data is pairs of different data types, e.g. text and image in CLIP (Contrastive Language-Image Pre-training). \n",
    "\n",
    "Model lears to generate similar embeddings to correct pairs and different embeddings for wrong pairs\n",
    "https://www.marqo.ai/course/introduction-to-clip-and-multimodal-models\n",
    "\n",
    "<img src=\"./images/f1-clip.png\" alt=\"Models\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146616a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to use Foundational models?\n",
    "\n",
    "1. Do not train them :D\n",
    "2. Give them text instructions: Prompt engineering\n",
    "3. Fine-tuning: Low-rank Adaptation (LoRA), domain adaptation\n",
    "\n",
    "Text input in foundational models is literally for **our instructions**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eccb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Foundational models are not omnipotent  \n",
    "https://www.scribbledata.io/blog/foundation-models-101-a-step-by-step-guide-for-beginners/\n",
    "\n",
    "<img src=\"./images/f1-limitations.jpg\" alt=\"Models\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd74a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Practical tips:\n",
    "\n",
    "* never ask a model to create a **whole** solution!\n",
    "* split solution into many simple steps\n",
    "* use AI models only where necessary; use basic code if you can\n",
    "* validate every model usage separately\n",
    "* create a system that tests different prompts or models; use best ones\n",
    "\n",
    "Domain adaptation is an open topic with no general solution. Be creative and check what others have done before."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
